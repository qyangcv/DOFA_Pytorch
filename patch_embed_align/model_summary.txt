DOFA-v2 ViT-Base Model Summary
==================================================

Total Parameters: 105,433,856
Model Size: 402.20 MB

Layer Details:
------------------------------
patch_embed.weight_generator.weight_tokens: [128, 128] (16,384 params)
patch_embed.weight_generator.bias_token: [1, 128] (128 params)
patch_embed.weight_generator.transformer_encoder.layers.0.self_attn.in_proj_weight: [384, 128] (49,152 params)
patch_embed.weight_generator.transformer_encoder.layers.0.self_attn.in_proj_bias: [384] (384 params)
patch_embed.weight_generator.transformer_encoder.layers.0.self_attn.out_proj.weight: [128, 128] (16,384 params)
patch_embed.weight_generator.transformer_encoder.layers.0.self_attn.out_proj.bias: [128] (128 params)
patch_embed.weight_generator.transformer_encoder.layers.0.linear1.weight: [2048, 128] (262,144 params)
patch_embed.weight_generator.transformer_encoder.layers.0.linear1.bias: [2048] (2,048 params)
patch_embed.weight_generator.transformer_encoder.layers.0.linear2.weight: [128, 2048] (262,144 params)
patch_embed.weight_generator.transformer_encoder.layers.0.linear2.bias: [128] (128 params)
patch_embed.weight_generator.transformer_encoder.layers.0.norm1.weight: [128] (128 params)
patch_embed.weight_generator.transformer_encoder.layers.0.norm1.bias: [128] (128 params)
patch_embed.weight_generator.transformer_encoder.layers.0.norm2.weight: [128] (128 params)
patch_embed.weight_generator.transformer_encoder.layers.0.norm2.bias: [128] (128 params)
patch_embed.weight_generator.fc_weight.weight: [150528, 128] (19,267,584 params)
patch_embed.weight_generator.fc_weight.bias: [150528] (150,528 params)
patch_embed.weight_generator.fc_bias.weight: [768, 128] (98,304 params)
patch_embed.weight_generator.fc_bias.bias: [768] (768 params)
patch_embed.fclayer.w1.weight: [128, 128] (16,384 params)
patch_embed.fclayer.w1.bias: [128] (128 params)
patch_embed.fclayer.w2.weight: [128, 128] (16,384 params)
patch_embed.fclayer.w2.bias: [128] (128 params)
model.cls_token: [1, 1, 768] (768 params)
model.pos_embed: [1, 257, 768] (197,376 params)
model.blocks.0.norm1.weight: [768] (768 params)
model.blocks.0.norm1.bias: [768] (768 params)
model.blocks.0.attn.qkv.weight: [2304, 768] (1,769,472 params)
model.blocks.0.attn.qkv.bias: [2304] (2,304 params)
model.blocks.0.attn.proj.weight: [768, 768] (589,824 params)
model.blocks.0.attn.proj.bias: [768] (768 params)
model.blocks.0.ls1.gamma: [768] (768 params)
model.blocks.0.norm2.weight: [768] (768 params)
model.blocks.0.norm2.bias: [768] (768 params)
model.blocks.0.mlp.fc1.weight: [3072, 768] (2,359,296 params)
model.blocks.0.mlp.fc1.bias: [3072] (3,072 params)
model.blocks.0.mlp.fc2.weight: [768, 3072] (2,359,296 params)
model.blocks.0.mlp.fc2.bias: [768] (768 params)
model.blocks.0.ls2.gamma: [768] (768 params)
model.blocks.1.norm1.weight: [768] (768 params)
model.blocks.1.norm1.bias: [768] (768 params)
model.blocks.1.attn.qkv.weight: [2304, 768] (1,769,472 params)
model.blocks.1.attn.qkv.bias: [2304] (2,304 params)
model.blocks.1.attn.proj.weight: [768, 768] (589,824 params)
model.blocks.1.attn.proj.bias: [768] (768 params)
model.blocks.1.ls1.gamma: [768] (768 params)
model.blocks.1.norm2.weight: [768] (768 params)
model.blocks.1.norm2.bias: [768] (768 params)
model.blocks.1.mlp.fc1.weight: [3072, 768] (2,359,296 params)
model.blocks.1.mlp.fc1.bias: [3072] (3,072 params)
model.blocks.1.mlp.fc2.weight: [768, 3072] (2,359,296 params)
model.blocks.1.mlp.fc2.bias: [768] (768 params)
model.blocks.1.ls2.gamma: [768] (768 params)
model.blocks.2.norm1.weight: [768] (768 params)
model.blocks.2.norm1.bias: [768] (768 params)
model.blocks.2.attn.qkv.weight: [2304, 768] (1,769,472 params)
model.blocks.2.attn.qkv.bias: [2304] (2,304 params)
model.blocks.2.attn.proj.weight: [768, 768] (589,824 params)
model.blocks.2.attn.proj.bias: [768] (768 params)
model.blocks.2.ls1.gamma: [768] (768 params)
model.blocks.2.norm2.weight: [768] (768 params)
model.blocks.2.norm2.bias: [768] (768 params)
model.blocks.2.mlp.fc1.weight: [3072, 768] (2,359,296 params)
model.blocks.2.mlp.fc1.bias: [3072] (3,072 params)
model.blocks.2.mlp.fc2.weight: [768, 3072] (2,359,296 params)
model.blocks.2.mlp.fc2.bias: [768] (768 params)
model.blocks.2.ls2.gamma: [768] (768 params)
model.blocks.3.norm1.weight: [768] (768 params)
model.blocks.3.norm1.bias: [768] (768 params)
model.blocks.3.attn.qkv.weight: [2304, 768] (1,769,472 params)
model.blocks.3.attn.qkv.bias: [2304] (2,304 params)
model.blocks.3.attn.proj.weight: [768, 768] (589,824 params)
model.blocks.3.attn.proj.bias: [768] (768 params)
model.blocks.3.ls1.gamma: [768] (768 params)
model.blocks.3.norm2.weight: [768] (768 params)
model.blocks.3.norm2.bias: [768] (768 params)
model.blocks.3.mlp.fc1.weight: [3072, 768] (2,359,296 params)
model.blocks.3.mlp.fc1.bias: [3072] (3,072 params)
model.blocks.3.mlp.fc2.weight: [768, 3072] (2,359,296 params)
model.blocks.3.mlp.fc2.bias: [768] (768 params)
model.blocks.3.ls2.gamma: [768] (768 params)
model.blocks.4.norm1.weight: [768] (768 params)
model.blocks.4.norm1.bias: [768] (768 params)
model.blocks.4.attn.qkv.weight: [2304, 768] (1,769,472 params)
model.blocks.4.attn.qkv.bias: [2304] (2,304 params)
model.blocks.4.attn.proj.weight: [768, 768] (589,824 params)
model.blocks.4.attn.proj.bias: [768] (768 params)
model.blocks.4.ls1.gamma: [768] (768 params)
model.blocks.4.norm2.weight: [768] (768 params)
model.blocks.4.norm2.bias: [768] (768 params)
model.blocks.4.mlp.fc1.weight: [3072, 768] (2,359,296 params)
model.blocks.4.mlp.fc1.bias: [3072] (3,072 params)
model.blocks.4.mlp.fc2.weight: [768, 3072] (2,359,296 params)
model.blocks.4.mlp.fc2.bias: [768] (768 params)
model.blocks.4.ls2.gamma: [768] (768 params)
model.blocks.5.norm1.weight: [768] (768 params)
model.blocks.5.norm1.bias: [768] (768 params)
model.blocks.5.attn.qkv.weight: [2304, 768] (1,769,472 params)
model.blocks.5.attn.qkv.bias: [2304] (2,304 params)
model.blocks.5.attn.proj.weight: [768, 768] (589,824 params)
model.blocks.5.attn.proj.bias: [768] (768 params)
model.blocks.5.ls1.gamma: [768] (768 params)
model.blocks.5.norm2.weight: [768] (768 params)
model.blocks.5.norm2.bias: [768] (768 params)
model.blocks.5.mlp.fc1.weight: [3072, 768] (2,359,296 params)
model.blocks.5.mlp.fc1.bias: [3072] (3,072 params)
model.blocks.5.mlp.fc2.weight: [768, 3072] (2,359,296 params)
model.blocks.5.mlp.fc2.bias: [768] (768 params)
model.blocks.5.ls2.gamma: [768] (768 params)
model.blocks.6.norm1.weight: [768] (768 params)
model.blocks.6.norm1.bias: [768] (768 params)
model.blocks.6.attn.qkv.weight: [2304, 768] (1,769,472 params)
model.blocks.6.attn.qkv.bias: [2304] (2,304 params)
model.blocks.6.attn.proj.weight: [768, 768] (589,824 params)
model.blocks.6.attn.proj.bias: [768] (768 params)
model.blocks.6.ls1.gamma: [768] (768 params)
model.blocks.6.norm2.weight: [768] (768 params)
model.blocks.6.norm2.bias: [768] (768 params)
model.blocks.6.mlp.fc1.weight: [3072, 768] (2,359,296 params)
model.blocks.6.mlp.fc1.bias: [3072] (3,072 params)
model.blocks.6.mlp.fc2.weight: [768, 3072] (2,359,296 params)
model.blocks.6.mlp.fc2.bias: [768] (768 params)
model.blocks.6.ls2.gamma: [768] (768 params)
model.blocks.7.norm1.weight: [768] (768 params)
model.blocks.7.norm1.bias: [768] (768 params)
model.blocks.7.attn.qkv.weight: [2304, 768] (1,769,472 params)
model.blocks.7.attn.qkv.bias: [2304] (2,304 params)
model.blocks.7.attn.proj.weight: [768, 768] (589,824 params)
model.blocks.7.attn.proj.bias: [768] (768 params)
model.blocks.7.ls1.gamma: [768] (768 params)
model.blocks.7.norm2.weight: [768] (768 params)
model.blocks.7.norm2.bias: [768] (768 params)
model.blocks.7.mlp.fc1.weight: [3072, 768] (2,359,296 params)
model.blocks.7.mlp.fc1.bias: [3072] (3,072 params)
model.blocks.7.mlp.fc2.weight: [768, 3072] (2,359,296 params)
model.blocks.7.mlp.fc2.bias: [768] (768 params)
model.blocks.7.ls2.gamma: [768] (768 params)
model.blocks.8.norm1.weight: [768] (768 params)
model.blocks.8.norm1.bias: [768] (768 params)
model.blocks.8.attn.qkv.weight: [2304, 768] (1,769,472 params)
model.blocks.8.attn.qkv.bias: [2304] (2,304 params)
model.blocks.8.attn.proj.weight: [768, 768] (589,824 params)
model.blocks.8.attn.proj.bias: [768] (768 params)
model.blocks.8.ls1.gamma: [768] (768 params)
model.blocks.8.norm2.weight: [768] (768 params)
model.blocks.8.norm2.bias: [768] (768 params)
model.blocks.8.mlp.fc1.weight: [3072, 768] (2,359,296 params)
model.blocks.8.mlp.fc1.bias: [3072] (3,072 params)
model.blocks.8.mlp.fc2.weight: [768, 3072] (2,359,296 params)
model.blocks.8.mlp.fc2.bias: [768] (768 params)
model.blocks.8.ls2.gamma: [768] (768 params)
model.blocks.9.norm1.weight: [768] (768 params)
model.blocks.9.norm1.bias: [768] (768 params)
model.blocks.9.attn.qkv.weight: [2304, 768] (1,769,472 params)
model.blocks.9.attn.qkv.bias: [2304] (2,304 params)
model.blocks.9.attn.proj.weight: [768, 768] (589,824 params)
model.blocks.9.attn.proj.bias: [768] (768 params)
model.blocks.9.ls1.gamma: [768] (768 params)
model.blocks.9.norm2.weight: [768] (768 params)
model.blocks.9.norm2.bias: [768] (768 params)
model.blocks.9.mlp.fc1.weight: [3072, 768] (2,359,296 params)
model.blocks.9.mlp.fc1.bias: [3072] (3,072 params)
model.blocks.9.mlp.fc2.weight: [768, 3072] (2,359,296 params)
model.blocks.9.mlp.fc2.bias: [768] (768 params)
model.blocks.9.ls2.gamma: [768] (768 params)
model.blocks.10.norm1.weight: [768] (768 params)
model.blocks.10.norm1.bias: [768] (768 params)
model.blocks.10.attn.qkv.weight: [2304, 768] (1,769,472 params)
model.blocks.10.attn.qkv.bias: [2304] (2,304 params)
model.blocks.10.attn.proj.weight: [768, 768] (589,824 params)
model.blocks.10.attn.proj.bias: [768] (768 params)
model.blocks.10.ls1.gamma: [768] (768 params)
model.blocks.10.norm2.weight: [768] (768 params)
model.blocks.10.norm2.bias: [768] (768 params)
model.blocks.10.mlp.fc1.weight: [3072, 768] (2,359,296 params)
model.blocks.10.mlp.fc1.bias: [3072] (3,072 params)
model.blocks.10.mlp.fc2.weight: [768, 3072] (2,359,296 params)
model.blocks.10.mlp.fc2.bias: [768] (768 params)
model.blocks.10.ls2.gamma: [768] (768 params)
model.blocks.11.norm1.weight: [768] (768 params)
model.blocks.11.norm1.bias: [768] (768 params)
model.blocks.11.attn.qkv.weight: [2304, 768] (1,769,472 params)
model.blocks.11.attn.qkv.bias: [2304] (2,304 params)
model.blocks.11.attn.proj.weight: [768, 768] (589,824 params)
model.blocks.11.attn.proj.bias: [768] (768 params)
model.blocks.11.ls1.gamma: [768] (768 params)
model.blocks.11.norm2.weight: [768] (768 params)
model.blocks.11.norm2.bias: [768] (768 params)
model.blocks.11.mlp.fc1.weight: [3072, 768] (2,359,296 params)
model.blocks.11.mlp.fc1.bias: [3072] (3,072 params)
model.blocks.11.mlp.fc2.weight: [768, 3072] (2,359,296 params)
model.blocks.11.mlp.fc2.bias: [768] (768 params)
model.blocks.11.ls2.gamma: [768] (768 params)
model.norm.weight: [768] (768 params)
model.norm.bias: [768] (768 params)
norm.weight: [768] (768 params)
norm.bias: [768] (768 params)
